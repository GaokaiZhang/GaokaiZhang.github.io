---
layout: default
title: "About Me"
permalink: /
---

#### An LLM researcher who gazes at the starlit skies of Artificial General Intelligence ✨

Ex‑intern at [Microsoft Research Asia (MSRA)](https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/) [Systems & Networking Group](https://www.microsoft.com/en-us/research/group/systems-research-group-asia/) mentored by [Dr. Li Lyna Zhang](https://www.microsoft.com/en-us/research/people/lzhani/).  
Previously research assistant at UIUC with [Prof. Fan Lai](https://www.fanlai.me/) and [Prof. Minjia Zhang](https://minjiazhang.github.io/).  
Incoming M.S. in [Intelligent Information Systems @ CMU LTI](https://miis.cs.cmu.edu/) in Fall 2025.
Open to MLE and SDE intern opportunities! 

---

## 📂 ([~/about_me/](/Gaokai_Resume.pdf))

Hi there! 👋 I’m **Gaokai Zhang**. I am an MIIS at CMU‑LTI since Fall 2025 (NLP/LLM) and I hold dual B.S. degrees at [ZJUI](https://zjui.intl.zju.edu.cn/en/) (CompE @ UIUC, ECE @ ZJU).

From July 2024 to July 2025, I joined Microsoft SRG group as an intern. At **MSRA**, I gained hands‑on experience with LLMs, RL, SFT, and business‑scale ML systems. I incorporated novel data synthesis and solid reinforcement learning in ([LoongRL](https://arxiv.org/abs/2510.19363)) to make 7B models surpass 32B LRMs in long-context reasoning tasks at even 100k-200k tokens.
I contributed **LongRoPE2**—extending LLM context windows to 128 K tokens while retaining 98.5 % short‑context accuracy ([ICML 2025 poster](https://arxiv.org/abs/2502.20082))—and built a parallel pipeline for large‑scale user‑query processing and delivered production‑ready long‑context recommendation models to [Microsoft Asia‑Pacific R&D](https://www.microsoft.com/en-us/aprd/default).

At **UIUC**, I have worked on:  
- Monte‑Carlo‑Tree‑Search planning for cost‑efficient LLM training on heterogeneous GPUs/TPUs.  
- Robustness benchmarking of LLMs ([Stochastic Monkeys](https://arxiv.org/abs/2411.02785)).  

---

## 🧠 what_drives_me.txt
- Long‑context reasoning & scaling  
- Cloud‑optimized training architectures  
- Fine-tuning and reinforcement learning  

Outside work I’ve logged **8 000 + h** gaming 🎮 (also a lifetime **Faker** fan), vibe to rap 🎧, and hunt the perfect **omakase** bite 🍣.  

*Feel free to reach out, always happy to connect with like‑minded friends and collaborators!*  

---

## 🏅 highlights.json
- 🎓 MIIS @ CMU LTI (2025 – 2027)  
- 🧪 ICML 2025 poster: *LongRoPE2* ([arXiv](https://arxiv.org/abs/2502.20082))  
- 💼 Research Intern @ MSRA
- 🏫 Dual B.S. CompE @ UIUC & ECE @ ZJU  
<!--- 🏅 1× recipient of China’s National Scholarship — awarded to the top 0.2% of university students nationwide-->  

---

## 📰 changelog.md
- 🧾 May 2025 – LongRoPE2 accepted at ICML 🎉  
- 🏢 Jul 2024 – Joined MSRA as intern  
- 🧑‍🔬 Mar 2024 – Started LLM projects @ UIUC  

---

## 📫 Contact
- 💼 [LinkedIn](https://linkedin.com/in/gaokaizhang)  
- 🐙 [GitHub](https://github.com/GaokaiZhang)  
- 📧 [gaokaiz2@andrew.cmu.edu](mailto:gaokaiz2@andrew.cmu.edu)
