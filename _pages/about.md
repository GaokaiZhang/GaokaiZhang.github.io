---
layout: about
title: about
permalink: /
subtitle: An LLM researcher who gazes at the starlit skies of Artificial General Intelligence

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false
  more_info: >
    <p>M.S. in Intelligent Information Systems</p>
    <p>Carnegie Mellon University</p>
    <p>Language Technologies Institute</p>

selected_papers: true
social: true

announcements:
  enabled: true
  scrollable: true
  limit: 5

latest_posts:
  enabled: false
---

Hi there! I'm **Gaokai Zhang**, an M.S. student in [Intelligent Information Systems](https://miis.cs.cmu.edu/) at CMU LTI since Fall 2025. I hold dual B.S. degrees from [ZJUI](https://zjui.intl.zju.edu.cn/en/) (CompE @ UIUC, ECE @ ZJU).

Currently, I'm working on **SWE-Bench related code-generation agent training** with [Kexun Zhang](https://zkx06111.github.io/) in [Prof. Lei Li's lab](https://leililab.github.io/), focusing on supervised fine-tuning and reinforcement learning for coding agents.

**Open to LLM-related MLE/RS opportunities as I'm graduating in December 2026!**

---

### Experience

**Microsoft Research Asia** (Jul 2024 - Jul 2025)
*Research Intern, Systems & Networking Group*
Mentored by [Dr. Li Lyna Zhang](https://www.microsoft.com/en-us/research/people/lzhani/)

- Led [**LoongRL**](https://arxiv.org/abs/2510.19363): Novel data synthesis + reinforcement learning enabling 7B models to surpass 32B LRMs in long-context reasoning (100k-200k tokens)
- Contributed to [**LongRoPE2**](https://arxiv.org/abs/2502.20082): Extended LLM context windows to 128K tokens while retaining 98.5% short-context accuracy (**ICML 2025 poster**)
- Built parallel pipeline for large-scale user-query processing; delivered production-ready long-context recommendation models to [Microsoft Asia-Pacific R&D](https://www.microsoft.com/en-us/aprd/default)

**University of Illinois Urbana-Champaign**
*Research Assistant* with [Prof. Fan Lai](https://www.fanlai.me/) and [Prof. Minjia Zhang](https://minjiazhang.github.io/)

- Monte-Carlo-Tree-Search planning for cost-efficient LLM training on heterogeneous GPUs/TPUs
- Robustness benchmarking of LLMs ([Stochastic Monkeys](https://arxiv.org/abs/2411.02785))

---

### Research Interests

- Long-context reasoning & scaling
- Reinforcement learning for LLMs
- Efficient training architectures
- Code generation agents

---

### Beyond Research

Outside work, I enjoy gaming (lifetime **Faker** fan), vibe to rap, and hunt for the perfect **omakase** bite.

*Feel free to reach out - always happy to connect with like-minded friends and collaborators!*
