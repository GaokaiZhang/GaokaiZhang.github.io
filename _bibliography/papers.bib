---
---

@article{zhang2025longrope2,
  abbr={ICML},
  title={LongRoPE2: Near-Lossless LLM Context Window Scaling},
  author={Wang, Ning and Wang, Shichen and Lv, Ao and Zhang, Gaokai and Xiao, Zipeng and Shi, Kan and Zhu, Jing and Yang, Zhong and Zhang, Yifan and Zhang, Li Lyna and Yang, Mao and Qiu, Xipeng},
  journal={International Conference on Machine Learning},
  year={2025},
  arxiv={2502.20082},
  selected={true},
  abstract={Large Language Models (LLMs) with extended context windows are essential for complex tasks. We present LongRoPE2, a novel method that extends LLM context windows to 128K tokens while retaining 98.5% short-context accuracy. Our approach introduces improved position encoding strategies that enable near-lossless context extension.},
  html={https://arxiv.org/abs/2502.20082}
}

@article{zhang2025loongrl,
  abbr={Preprint},
  title={LoongRL: Incentivizing Long-Context Reasoning in Large Language Models via Reinforcement Learning},
  author={Zhang, Gaokai and Zhang, Shichen and others},
  journal={arXiv preprint},
  year={2025},
  arxiv={2510.19363},
  selected={true},
  abstract={We present LoongRL, a reinforcement learning framework with novel data synthesis that enables 7B parameter models to surpass 32B long-range models on long-context reasoning tasks at 100k-200k tokens.},
  html={https://arxiv.org/abs/2510.19363}
}

@article{huang2024stochastic,
  abbr={Preprint},
  title={Stochastic Monkeys at Play: Random Augmentations Cheaply Break LLM Safety Alignment},
  author={Huang, Jason and others and Zhang, Gaokai and others},
  journal={arXiv preprint},
  year={2024},
  arxiv={2411.02785},
  selected={true},
  abstract={We present a comprehensive robustness benchmarking study of Large Language Models, demonstrating that simple random augmentations can effectively break safety alignments.},
  html={https://arxiv.org/abs/2411.02785}
}
