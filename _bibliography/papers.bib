---
---

@article{zhang2025longrope2,
  abbr={ICML 2025},
  title={LongRoPE2: Near-Lossless LLM Context Window Scaling},
  author={Shang, Ning and Zhang, Li Lyna and Wang, Siyuan and Zhang, Gaokai and Lopez, Gilsinia and Yang, Fan and Chen, Weizhu and Yang, Mao},
  journal={International Conference on Machine Learning},
  year={2025},
  arxiv={2502.20082},
  selected={true},
  abstract={Large Language Models (LLMs) with extended context windows are essential for complex tasks. We present LongRoPE2, a novel method that extends LLM context windows to 128K tokens while retaining 98.5% short-context accuracy. Our approach introduces improved position encoding strategies that enable near-lossless context extension.},
  html={https://arxiv.org/abs/2502.20082}
}

@article{zhang2025loongrl,
  abbr={ICLR 2026},
  title={LoongRL: Incentivizing Long-Context Reasoning in Large Language Models via Reinforcement Learning},
  author={Wang, Siyuan and Zhang, Gaokai and Zhang, Li Lyna and Shang, Ning and Yang, Fan and Chen, Dongyao and Yang, Mao},
  journal={International Conference on Learning Representations},
  year={2026},
  arxiv={2510.19363},
  selected={true},
  abstract={We present LoongRL, a reinforcement learning framework with novel data synthesis that enables 7B parameter models to surpass 32B long-range models on long-context reasoning tasks at 100k-200k tokens.},
  html={https://arxiv.org/abs/2510.19363}
}

@article{vega2024stochastic,
  abbr={Preprint},
  title={Stochastic Monkeys at Play: Random Augmentations Cheaply Break LLM Safety Alignment},
  author={Vega, Jason and Huang, Junsheng and Zhang, Gaokai and Kang, Hangoo and Zhang, Minjia and Singh, Gagandeep},
  journal={arXiv preprint},
  year={2024},
  arxiv={2411.02785},
  selected={true},
  abstract={We present a comprehensive robustness benchmarking study of Large Language Models, demonstrating that simple random augmentations can effectively break safety alignments.},
  html={https://arxiv.org/abs/2411.02785}
}
